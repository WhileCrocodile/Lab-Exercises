{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2c14b7",
   "metadata": {},
   "source": [
    "Part I: Completing the third Selenium Tutorial (5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f3a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Defines the Chrome driver we use to access the web\n",
    "PATH = Service(\"C:\\Program Files (x86)\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=PATH)\n",
    "\n",
    "# The Chrome browser opens the page https://www.techwithtim.net/ \n",
    "driver.get(\"https://www.techwithtim.net/\")\n",
    "\n",
    "# Chrome finds the element with the text \"Python Programming\", and clicks on it\n",
    "link = driver.find_element(By.LINK_TEXT, \"Python Programming\")\n",
    "link.click()\n",
    "\n",
    "try:\n",
    "    # Chrome waits for the element/link with the text \"Beginner Python Tutorials\" to load, then clicks on it\n",
    "    # If we were accessing a search bar, element.clear() would clear the text input field\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.LINK_TEXT, \"Beginner Python Tutorials\"))\n",
    "    )\n",
    "    # element.clear()\n",
    "    element.click()\n",
    "    \n",
    "    # Chrome waits for the unique element \"sow-button-19310003\" to load, then clicks on it\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"sow-button-19310003\"))\n",
    "    )\n",
    "    element.click()\n",
    "    \n",
    "    # Chrome goes back three pages, the goes forward two pages to land on the \"Beginner Python Tutorials Page\"\n",
    "    driver.back()\n",
    "    driver.back()\n",
    "    driver.back()\n",
    "    driver.forward()\n",
    "    driver.forward()\n",
    "\n",
    "# If an error is encountered at any part of the process, quit the browser\n",
    "except:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be095d64",
   "metadata": {},
   "source": [
    "Part II: Scraping https://data.gov/ for climate change search results (5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64f4c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Accesses our CSV file in advance\n",
    "csv_file = open(\"data_gov_climate_change.csv\", \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow([\"title\", \"department\", \"desc\"])\n",
    "\n",
    "PATH = Service(\"C:\\Program Files (x86)\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=PATH)\n",
    "\n",
    "driver.get(\"https://data.gov/\")\n",
    "\n",
    "try:\n",
    "    search =  WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"search-header\"))\n",
    "    )\n",
    "    search.clear()\n",
    "    search.send_keys(\"climate change\")\n",
    "    search.send_keys(Keys.RETURN)\n",
    "    \n",
    "    results_container =  WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"dataset-list\"))\n",
    "    )\n",
    "    \n",
    "    results = results_container.find_elements(By.CLASS_NAME, \"dataset-content\")\n",
    "    \n",
    "    for result in results:\n",
    "        title = result.find_element(By.TAG_NAME, \"a\").text\n",
    "        desc = result.find_element(By.CLASS_NAME, \"notes\")\n",
    "        department = desc.find_element(By.CLASS_NAME, \"dataset-organization\").text.strip(\" â€”\")\n",
    "        desc_text = desc.find_element(By.TAG_NAME, \"div\").text\n",
    "        \n",
    "        csv_writer.writerow([title, department, desc_text])\n",
    "    \n",
    "except:\n",
    "    driver.quit()\n",
    "    \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27c611",
   "metadata": {},
   "source": [
    "Part III (Bonus): Scraping the Supreme Court website for Court Opinions (2pts)\n",
    "\n",
    "Note: Wow, this was quite hard! I must have spent 2-3 hours just working out the kinks in this. I'm not sure if this was worth the extra 2 points, but it was definitely worth the experience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fb9ef53c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Accesses our Supreme Court Opinions csv\n",
    "csv_file = open(\"supreme_court_opinions.csv\", \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow([\"date\", \"docket\", \"name\", \"volume\", \"link\"])\n",
    "\n",
    "# Defines the Chrome driver we use to access the web\n",
    "PATH = Service(\"C:\\Program Files (x86)\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=PATH)\n",
    "\n",
    "# Defines the range of years (therefore pages) which we will scrape\n",
    "starting_year = \"2016\"\n",
    "ending_year = \"2022\"\n",
    "\n",
    "for year in range(int(starting_year[2:4]), int(ending_year[2:4])+1):\n",
    "    driver.get(f\"https://www.supremecourt.gov/opinions/slipopinion/{year}\")\n",
    "    \n",
    "    try:\n",
    "        # Finds the container which holds all the tables\n",
    "        all_cases =  WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"list\"))\n",
    "        )\n",
    "\n",
    "        # Opens all collapsible tables by clicking on them, so their data can be read\n",
    "        # Skips the first table, which is open by default\n",
    "        table_toggles = all_cases.find_elements(By.XPATH, \"//a[@class='accordion-toggle']\")\n",
    "        for toggle in table_toggles[1:]:\n",
    "            toggle.click()\n",
    "            time.sleep(0.3)\n",
    "\n",
    "        # Finds all of the tables in the overarching container\n",
    "        tables = all_cases.find_elements(By.CLASS_NAME, \"table\")\n",
    "\n",
    "        # Breaks down the tables into rows and columns, and enters each column as data into the CSV\n",
    "        for table in tables:\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            for row in rows:\n",
    "                columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "\n",
    "                # If the columns are empty, it must be a header row, therefore skip\n",
    "                if columns == []:\n",
    "                    continue\n",
    "\n",
    "                # This iterable lets us keep track of which column we're on\n",
    "                column_pointer = iter(range(7))\n",
    "                for column in columns:\n",
    "                    current_column = next(column_pointer)\n",
    "\n",
    "                    # Depending on which column we're on, assign it to a different variable\n",
    "                    if current_column == 1:\n",
    "                        date = column.text\n",
    "                    elif current_column == 2:\n",
    "                        docket = column.text\n",
    "                    elif current_column == 3:\n",
    "                        name = column.text\n",
    "                        link = column.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                    elif current_column == 6:\n",
    "                        volume = column.text\n",
    "                        \n",
    "                # Writes our data into our CSV\n",
    "                csv_writer.writerow([date, docket, name, volume, link])\n",
    "        \n",
    "    except:\n",
    "        print(\"Some error has occured!\")\n",
    "        driver.quit()\n",
    "\n",
    "csv_file.close()\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
